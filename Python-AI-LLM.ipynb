{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPI6WRaVL9ClloMLHVv9/87",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aknip/Coding-Cheatsheets/blob/main/Python-AI-LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0O1ck0sc0m2v"
      },
      "outputs": [],
      "source": [
        "!pip install openai tiktoken litellm langchain icecream --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from getpass import getpass\n",
        "import psutil\n",
        "import requests\n",
        "import textwrap\n",
        "from icecream import ic\n",
        "import time\n",
        "IN_NOTEBOOK = any([\"jupyter-notebook\" in i for i in psutil.Process().parent().cmdline()])\n",
        "if IN_NOTEBOOK:\n",
        "  CREDS = json.loads(getpass(\"Secrets (JSON string): \"))\n",
        "  os.environ['CREDS'] = json.dumps(CREDS)\n",
        "  CREDS = json.loads(os.getenv('CREDS'))"
      ],
      "metadata": {
        "id": "myZYsW9oZkEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from litellm import completion\n",
        "os.environ[\"OPENAI_API_KEY\"] = CREDS['OpenAI']['v1']['credential'] # my key\n",
        "os.environ[\"TOGETHERAI_API_KEY\"] = CREDS['together-ai']['key']['credential']\n",
        "os.environ['ANTHROPIC_API_KEY'] = CREDS['anthropic']['key']['credential']"
      ],
      "metadata": {
        "id": "y0pi_2z0ZsOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple LLM calls"
      ],
      "metadata": {
        "id": "_N2G8OUkzXLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "\n",
        "client = openai.OpenAI(\n",
        "  api_key=os.environ.get(\"TOGETHERAI_API_KEY\"),\n",
        "  base_url='https://api.together.xyz',\n",
        ")\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"Hello, how are you?\",\n",
        "    }\n",
        "  ],\n",
        "  model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "  max_tokens=1024\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "Ku-bklBeaTpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call via lite-llm\n",
        "\n",
        "response = completion(\n",
        "  #model=\"together_ai/togethercomputer/Llama-2-7B-32K-Instruct\",\n",
        "  model=\"together_ai/mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "  messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}]\n",
        ")\n",
        "print(response.choices[0].message.content)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "PoaFpFXKc8zE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = completion(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}]\n",
        ")\n",
        "print(response.choices[0].message.content)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "l4QNQHwhc38u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function Calling\n",
        "\n",
        "# see https://litellm.vercel.app/docs/completion/function_call\n",
        "\n",
        "# via Huggingface?\n",
        "# https://litellm.vercel.app/docs/providers/huggingface\n",
        "# https://huggingface.co/Trelis/Mixtral-8x7B-Instruct-v0.1-function-calling-v3\n",
        "# https://huggingface.co/Trelis/Mistral-7B-Instruct-v0.1-function-calling-v2\n",
        "\n",
        "# via Anyscale?\n",
        "# https://docs.litellm.ai/docs/providers/anyscale\n",
        "# https://www.anyscale.com/blog/anyscale-endpoints-json-mode-and-function-calling-features\n",
        "\n",
        "import os, litellm\n",
        "from litellm import completion\n",
        "\n",
        "# IMPORTANT - Set this to TRUE to add the function to the prompt for Non OpenAI LLMs\n",
        "litellm.add_function_to_prompt = True\n",
        "\n",
        "# The real function is not needed for the LLM. It may be called after the LLM call (not in this code!)\n",
        "def get_current_weather(location):\n",
        "  if location == \"Boston, MA\":\n",
        "    return \"The weather is 12F\"\n",
        "\n",
        "functions = [\n",
        "    {\n",
        "      \"name\": \"get_current_weather\",\n",
        "      \"description\": \"Get the current weather in a given location\",\n",
        "      \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "          \"location\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
        "          },\n",
        "          \"unit\": {\n",
        "            \"type\": \"string\",\n",
        "            \"enum\": [\"celsius\", \"fahrenheit\"]\n",
        "          }\n",
        "        },\n",
        "        \"required\": [\"location\"]\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What is the weather like in Boston?\"}\n",
        "]\n",
        "\n",
        "llm_model = \"gpt-3.5-turbo\"\n",
        "#llm_model = \"gpt-3.5-turbo-1106\"\n",
        "#llm_model = \"gpt-4\"\n",
        "#llm_model = \"claude-2\" # ??? claude answers in response.choices[0]['message']['content']\n",
        "#llm_model = \"together_ai/mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "\n",
        "\n",
        "response = completion(model=llm_model, messages=messages, functions=functions)\n",
        "\n",
        "function_found = hasattr(response.choices[0]['message'], 'function_call')\n",
        "if function_found == True:\n",
        "  function_call = response.choices[0]['message']['function_call']\n",
        "  function_call_name = function_call.name\n",
        "  function_call_arguments = function_call.arguments\n",
        "  print(function_call_arguments)\n",
        "else:\n",
        "  if llm_model == \"claude-2\":\n",
        "    print(response.choices[0]['message']['content'])\n",
        "  else:\n",
        "    print('No function found')\n",
        "\n",
        "print('\\n')\n",
        "print(response.choices[0]['message']['content'])\n",
        "print('\\n')\n",
        "print(json.dumps(response.json(), indent=2))"
      ],
      "metadata": {
        "id": "TG6qj6W282Dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, litellm\n",
        "from litellm import completion\n",
        "\n",
        "litellm.add_function_to_prompt = False\n",
        "\n",
        "functions_string = \"\"\"[\n",
        "  {\n",
        "    \"name\": \"get_risk_data\",\n",
        "    \"description\": \"Daten zum Versicherungsnehmer und seiner Risiken\",\n",
        "    \"parameters\": {\n",
        "      \"type\": \"object\",\n",
        "      \"properties\": {\n",
        "        \"Versicherungssumme\": {\n",
        "          \"type\": \"integer\",\n",
        "          \"description\": \"Versicherungssumme oder Deckungssumme der Versicherung, z. B. 100000\"\n",
        "        },\n",
        "        \"Versicherungsnehmer_Name\": {\n",
        "          \"type\": \"string\",\n",
        "          \"description\": \"Name des versicherten Unternehmens, z. B. 'Testfirma GmbH'\"\n",
        "        },\n",
        "        \"Versicherungsnehmer_Anschrift\": {\n",
        "          \"type\": \"string\",\n",
        "          \"description\": \"Anschrift des versicherten Unternehmens: Strasse, PLZ, Ort, z. B. 'Musterstr. 10, 10000 Berlin'\"\n",
        "        },\n",
        "        \"Versicherungsnehmer_Branche\": {\n",
        "          \"type\": \"string\",\n",
        "          \"description\": \"Branche bzw. Tätigkeitsbereich des versicherten Unternehmens, z. B. 'Textilproduktion'\"\n",
        "        },\n",
        "        \"Versicherungsnehmer_Umsatz\": {\n",
        "          \"type\": \"string\",\n",
        "          \"description\": \"Jahresumsatz des versicherten Unternehmens, z. B. 23000000\"\n",
        "        },\n",
        "        \"Vorschäden_benannt\": {\n",
        "          \"type\": \"string\",\n",
        "          \"description\": \"Informationen zu früheren Schadensfällen, z. B. 'Ja, in 2019 gab es einen Schaden mit folgenden Details'\"\n",
        "        },\n",
        "      },\n",
        "      \"required\": [\"Versicherungssumme\"]\n",
        "    }\n",
        "  }\n",
        "]\n",
        "\"\"\"\n",
        "\n",
        "functions=eval(functions_string)\n",
        "\n",
        "email_test_text = \"\"\"wir hoffen, Sie hatten einen angenehmen Feiertag.\n",
        "\n",
        "Zu unserer besteMeiermen Kundenverbindung Zorrsen Beteiligungsgesellschaft mbH bitten wir um Ihre Quotierung zur D&O-Versicherung per 01.01.2024.\n",
        "\n",
        "Die aktuelle Versicherungssumme beträgt 5,5 Mio. € zu einer Jahresprämie in Höhe von 7.920 € netto.\n",
        "Der Umsatz im letzten Jahr betrug 78,5 Millionen EUR.\n",
        "\n",
        "Beigefügt erhalten Sie die bisherigen Vertragsunterlagen sowie ein Organigramm. Versichert werden soll die Zorrsen Beteiligungsgesellschaft mbH als VN und die darunterliegenden Tochtergesellschaften (gelb markiert).\n",
        "\n",
        "Es gibt einen laufenden D&O-Schadensfall aufgrund einer Inanspruchnahme eines ehemaligen Geschäftsführers, welcher im Jahr 2017 entlassen wurde. Ihm wird vorgeworfen, für ein damaliges Bauvorhaben das Budget deutlich überzogen zu haben.\n",
        "\n",
        "Für Ihre Rückmeldung vielen Dank im Voraus.\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Extrahiere die Risikodaten: \" + email_test_text}\n",
        "]\n",
        "\n",
        "llm_model = \"gpt-3.5-turbo\"\n",
        "#llm_model = \"gpt-3.5-turbo-1106\"\n",
        "#llm_model = \"gpt-4\"\n",
        "#llm_model = \"claude-2\" # ??? claude answers in response.choices[0]['message']['content']\n",
        "#llm_model = \"together_ai/mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "\n",
        "\n",
        "response = completion(model=llm_model, messages=messages, functions=functions)\n",
        "\n",
        "function_found = hasattr(response.choices[0]['message'], 'function_call')\n",
        "if function_found == True:\n",
        "  function_call = response.choices[0]['message']['function_call']\n",
        "  function_call_name = function_call.name\n",
        "  function_call_arguments = function_call.arguments\n",
        "  print(function_call_arguments)\n",
        "else:\n",
        "  if llm_model == \"claude-2\":\n",
        "    print(response.choices[0]['message']['content'])\n",
        "  else:\n",
        "    print('No function found')\n",
        "\n",
        "print('\\n')\n",
        "print(response.choices[0]['message']['content'])\n",
        "print('\\n')\n",
        "print(json.dumps(response.json(), indent=2))"
      ],
      "metadata": {
        "id": "AP0kOug2Vxh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.choices[0]['message']['content'])"
      ],
      "metadata": {
        "id": "9NNvVCYTQJuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text splitting"
      ],
      "metadata": {
        "id": "vSWhfyvFXDbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "content = \"This is a piece of text. With some text and some more text.\"\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 10,\n",
        "    chunk_overlap  = 0,\n",
        "    length_function = len,\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_text(content)\n",
        "\n",
        "print('Number of text chunks: ', len(chunks))\n",
        "\n",
        "for chunk in chunks:\n",
        "    print(chunk)"
      ],
      "metadata": {
        "id": "vLzlvIiwODXn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "170d881a-2205-4e68-90b8-01693919d8b1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of text chunks:  7\n",
            "This is a\n",
            "piece of\n",
            "text.\n",
            "With some\n",
            "text and\n",
            "some more\n",
            "text.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Token counting\n",
        "\n",
        "- https://platform.openai.com/tokenizer\n",
        "- https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb"
      ],
      "metadata": {
        "id": "tadgzYaZYgMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "content = \"This is a piece of text. With some text and some more text.\"\n",
        "\n",
        "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "token_count = len(enc.encode(content))\n",
        "\n",
        "print('Number of tokens: ', token_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVlFUHhXXwQ6",
        "outputId": "bcc16f12-ff6e-49bd-8655-2af769ec8325"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens:  15\n"
          ]
        }
      ]
    }
  ]
}