{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/aknip/Coding-Cheatsheets/blob/main/Python-Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0WxaejUDCple"
   },
   "source": [
    "# Python basics\n",
    "\n",
    "- Installs everything and prompts for secrets in first run\n",
    "- Avoids this in all subsequent runs (no redudant installations, optimized for speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s_oLTkSkCl8_"
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "import psutil\n",
    "IN_NOTEBOOK = any([\"jupyter-notebook\" in i for i in psutil.Process().parent().cmdline()])\n",
    "if IN_NOTEBOOK:\n",
    "  try:\n",
    "      # imports\n",
    "      from playwright.async_api import async_playwright\n",
    "      from playwright.sync_api import Page, expect\n",
    "      from loguru import logger\n",
    "  except ImportError:\n",
    "      !pip install pytest-playwright loguru --quiet\n",
    "      !playwright install\n",
    "      !pip install icecream dateparser re --quiet\n",
    "      # same imports as above...\n",
    "      from playwright.async_api import async_playwright\n",
    "      from playwright.sync_api import Page, expect\n",
    "      from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys, os, shutil\n",
    "import asyncio\n",
    "import atexit\n",
    "from getpass import getpass\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8V_iUfOEr3m4"
   },
   "source": [
    "# Virtual Environement mit virtualenv\n",
    "\n",
    "- uv\n",
    "\t- Install: `curl -LsSf https://astral.sh/uv/install.sh | sh`\n",
    "\t- uv venv  # Create a virtual environment in current dir in folder .venv.\n",
    "\t- source .venv/bin/activate # activate\n",
    "\t- Deactivate: `deactivate`\n",
    "\n",
    "- pyenv (verschiedene Python-Versionen + Virtual Environments)\n",
    "\t- Install pyenv\n",
    "\t\t- brew install pyenv\n",
    "\t\t- brew install pyenv-virtualenv\n",
    "\t\t- then shell config: https://github.com/pyenv/pyenv?tab=readme-ov-file#set-up-your-shell-environment-for-pyenv\n",
    "\t- Install Python version X\n",
    "\t\t- pyenv install 3.11.9\n",
    "\t\t- pyenv versions\n",
    "  - Switch to Pyhton version X\n",
    "\t\t-\tpyenv local 3.11.9\n",
    "\t\t- pyenv global 3.11.9\n",
    "\t- Check Python version:\n",
    "\t\t- python3 --version\n",
    "\t- Switch to main / system's Python version\n",
    "\t\t- pyenv local system\n",
    "\t\t- pyenv global system\n",
    "- Create virtualenv with name \"myenv\"\n",
    "\t\t- pyenv virtualenv 3.11.9 myenv\n",
    "\t\t-\tpyenv activate myenv\n",
    "\t\t- pyenv deactivate\n",
    "\t\t- pyenv virtualenv-delete myenv\n",
    "\n",
    "- Virtualenv\n",
    "\t- Install: `pip install --user virtualenv`\n",
    "\t- Create new environment:\n",
    "\t\t- Create project folder, go into it\n",
    "\t\t- then `python3 -m virtualenv myenv`\n",
    "\t\t- or `python3 -m virtualenv --python=python3.10 myenv`\n",
    "\t- Activate:  `source myenv/bin/activate`\n",
    "\t- Check:\n",
    "\t\t- python3 --version\n",
    "\t\t- pip list\n",
    "\t- Deactivate: `deactivate`\n",
    "\t- **use `%pip` and NOT `!pip`in Notebooks to install inside the current virutalenv!**\n",
    "\t- Details: https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments\n",
    "- Pyenv\n",
    "\t- kann auch andere Python Versionen nutzen?\n",
    "\t- Beispielprojekt: https://github.com/Forethought-Technologies/AutoChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDHluAOTsKf9"
   },
   "source": [
    "# Dependencies mit uv\n",
    "- Install: `curl -LsSf https://astral.sh/uv/install.sh | sh`\n",
    "- uv pip install lib1 lib2 lib3\n",
    "- uv pip install -r requirements.txt\n",
    "- all other pip commands (show, list, freeze...) are supported, too\n",
    "\n",
    "# Dependencies mit pip\n",
    "- Dependencies installieren:\n",
    "\t- Mit  `pip install lib1 lib2 lib3 --quiet` etc. mehrere Libraries installieren\n",
    "\t- Alternativ: `pip install -r requirements.txt`\n",
    "\t- Das file `requirements.txt` inkl. Versionsnummern kann per `pip freeze > requirements.txt` erzeugt werden\n",
    "- `pip show lib1` zeigt Version und lokale location der Library\n",
    "- `pip list` zeigt alle installierten Libraries\n",
    "- Alle Module deinstallieren:\n",
    "\t- pip freeze > dependencies.txt\n",
    "\t- pip uninstall -y -r dependencies.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4EeGdrLC7m7"
   },
   "source": [
    "# Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "neWYYdZ3C6S2"
   },
   "outputs": [],
   "source": [
    "if IN_NOTEBOOK:\n",
    "  try: CREDS\n",
    "  except NameError:\n",
    "    CREDS = json.loads(getpass(\"Secrets (JSON string): \"))\n",
    "    os.environ['CREDS'] = json.dumps(CREDS)\n",
    "    CREDS = json.loads(os.getenv('CREDS'))\n",
    "os.environ[\"OPENAI_API_KEY\"] = CREDS['OpenAI']['v1']['credential'] # my key\n",
    "os.environ[\"TOGETHERAI_API_KEY\"] = CREDS['together-ai']['key']['credential']\n",
    "os.environ['ANTHROPIC_API_KEY'] = CREDS['anthropic']['key']['credential']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJsYESKF4iMK"
   },
   "source": [
    "# Debugging and Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-c_aX4jpCajB",
    "outputId": "47c22a36-657c-40a9-f750-ba964c2b91f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log level 'CUSTOM' already defined.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj_folder = 'project'\n",
    "\n",
    "# Delete / create directory\n",
    "if os.path.exists(proj_folder):\n",
    "    shutil.rmtree(proj_folder)\n",
    "os.mkdir(proj_folder)\n",
    "\n",
    "try:\n",
    "  logger.level(\"CUSTOM\", no=22, color=\"<black>\", icon=\"@\")\n",
    "except:\n",
    "  print(\"Log level 'CUSTOM' already defined.\")\n",
    "logger.remove()  # Remove all handlers added so far, including the default one.\n",
    "logger.add(sys.stderr, level=\"TRACE\", format=\"{time:HH:mm:ss} | <level>{level: <10}</level> | {message}\")\n",
    "logger.add(proj_folder + \"/log_all.log\", level=\"TRACE\", format=\"{time:HH:mm:ss} | {level: <10} | {message}\")\n",
    "logger.add(proj_folder + \"/log_success.log\", level=\"SUCCESS\", format=\"{time:HH:mm:ss} | {level: <10} | {message}\")\n",
    "logger.add(proj_folder + \"/log_error.log\", level=\"ERROR\", format=\"{time:HH:mm:ss} | {level: <10} | {message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TU_-cdHaCj0N",
    "outputId": "30e410fc-a521-425f-b056-61e838367a39"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:24:46 | \u001b[36m\u001b[1mTRACE     \u001b[0m | A trace message.\n",
      "11:24:46 | \u001b[34m\u001b[1mDEBUG     \u001b[0m | A debug message.\n",
      "11:24:46 | \u001b[1mINFO      \u001b[0m | An info message.\n",
      "11:24:46 | \u001b[32m\u001b[1mSUCCESS   \u001b[0m | A success message.\n",
      "11:24:46 | \u001b[33m\u001b[1mWARNING   \u001b[0m | A warning message.\n",
      "11:24:46 | \u001b[31m\u001b[1mERROR     \u001b[0m | An error message.\n",
      "11:24:46 | \u001b[41m\u001b[1mCRITICAL  \u001b[0m | A critical message.\n",
      "11:24:46 | \u001b[30mCUSTOM    \u001b[0m | A custom level.\n"
     ]
    }
   ],
   "source": [
    "logger.trace(\"A trace message.\")\n",
    "logger.debug(\"A debug message.\")\n",
    "logger.info(\"An info message.\")\n",
    "logger.success(\"A success message.\")\n",
    "logger.warning(\"A warning message.\")\n",
    "logger.error(\"An error message.\")\n",
    "logger.critical(\"A critical message.\")\n",
    "logger.log(\"CUSTOM\", \"A custom level.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Or6hN3YkDpcn",
    "outputId": "50f87620-c278-41c5-934f-729e09afc9ab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:24:48 | \u001b[1mINFO      \u001b[0m | An info message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An info message.\n"
     ]
    }
   ],
   "source": [
    "# custom function as log handler - will be executed with every log call\n",
    "\n",
    "async def custom_function(msg):\n",
    "  msg_obj = json.loads(msg)\n",
    "  message = msg_obj['record']['message'] #print(msg_obj['text'])\n",
    "  level = msg_obj['record']['level']['name']\n",
    "  print(message)\n",
    "logger.add(custom_function, format=\"{level} | {message}\", serialize=True)\n",
    "\n",
    "logger.info(\"An info message.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hGF0Bugv4fGx",
    "outputId": "3bc726cb-611e-4f6a-d822-c4ae1c15f1be"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| foo(123): 456\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "456"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do not use print() - use ic() instead !\n",
    "\n",
    "from icecream import ic\n",
    "def foo(i):\n",
    "    return i + 333\n",
    "ic(foo(123))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMJnH7WmaicS"
   },
   "source": [
    "# Code structure, imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDwKyzzeam1Q"
   },
   "outputs": [],
   "source": [
    "# given: local file helper_functions.py\n",
    "\n",
    "import helper_functions # => imports AND executes file\n",
    "\n",
    "from helper_functions import function1, function2 # => imports functions (defined with def:), DOES NOT execute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJx8vTFeL749"
   },
   "source": [
    "# Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M9eHdMU7LQPA"
   },
   "outputs": [],
   "source": [
    "# Langen String im Code mit Backslash auf mehere Zeilen aufteilen\n",
    "long_string = 'Hier die ersten Wörter\\\n",
    " und hier die nächsten. Wichtig:\\\n",
    " Hinter dem Backslash kein SPACE!'\n",
    "\n",
    "\n",
    "# Die ersten 10 Zeichen eines Strings\n",
    "short_string = long_string[:10]\n",
    "\n",
    "# Langen String (ohne Zeilenumbrüche) ausgeben mit Zeilenumbruch bei 80 Zeichen\n",
    "import textwrap\n",
    "print(textwrap.fill(long_string, 80))\n",
    "# Langer String MIT Zeilenumbrüchen\n",
    "for x in long_string.split('\\n'):\n",
    "  print(textwrap.fill(x, 80))\n",
    "\n",
    "# Text in Array splitten - Trennzeichen doppelter Zeilenumbruch\n",
    "paragraphs_array = fulltext.split('\\n\\n')\n",
    "\n",
    "# Array mit Texten in einen String konvertieren, doppelter Zeilenumbruch als Trenner\n",
    "fulltext_from_array = '\\n\\n'.join(paragraphs_array)\n",
    "\n",
    "# Zahl in String wandeln, mit 3 führenden Nullen\n",
    "number_string = str(number).zfill(3) # 007\n",
    "\n",
    "# Datum / Uhrzeit als String\n",
    "from datetime import datetime\n",
    "current_date_time = datetime.now().strftime(\"%d.%m.%Y, %H:%M:%S\")\n",
    "filename = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# 15.08.2022, 17:49:46\n",
    "\n",
    "\n",
    "# String für Filenamen optimieren (Leerzeichen und Sonderzeichen entfernen)\n",
    "filename = \"This is not/ valid!-123\"\n",
    "clean_filename = \"\".join( x for x in filename if (x.isalnum() or x in \"._-\"))\n",
    "print(clean_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LaSCaCCc179H"
   },
   "source": [
    "# Templating with Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b-tHRbjf19zL"
   },
   "outputs": [],
   "source": [
    "# Nested templates\n",
    "\n",
    "# Variant 1:\n",
    "template = 'Hello, {var1} {var2}.'\n",
    "var1 = 'John'\n",
    "var2 = 'Doe'\n",
    "result1 = template.format(var1=var1, var2=var2)\n",
    "# Hello, John Doe.\n",
    "\n",
    "# Variant 2:\n",
    "def fstr(template):\n",
    "    return eval(f\"f'{template}'\")\n",
    "result2 = fstr(template)\n",
    "# Hello, John Doe.\n",
    "\n",
    "result3 = f\"{template}\" # does not work\n",
    "result4 = f\"Hello {var1}\" # works\n",
    "\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format JSON Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m{\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"color\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"red\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m},\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m{\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"color\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"blue\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m}\u001b[37m\u001b[39;49;00m\n",
      "]\u001b[37m\u001b[39;49;00m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pygments import highlight, lexers, formatters\n",
    "\n",
    "response_JSON = [{\"color\": \"red\"}, {\"color\": \"blue\"}]\n",
    "formatted_json = json.dumps(response_JSON, indent=4)\n",
    "colorful_json = highlight(formatted_json, lexers.JsonLexer(), formatters.TerminalFormatter())\n",
    "\n",
    "print(colorful_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mex_uPR4L3Px"
   },
   "source": [
    "# Date and time / Formatting / Measure time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1vnvbWE_1I8t",
    "outputId": "20ddd205-7df3-4a2d-a477-5e3761fefc8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-25 08:56:55.050608\n",
      "2024-01-24 08:56:55.050608\n",
      "2024-01-22\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from pandas.tseries.offsets import BDay\n",
    "# Heute\n",
    "today = datetime.datetime.today()\n",
    "print(today)\n",
    "# Lezter Werktag (wenn heute = MO wäre das FR)\n",
    "print(today - BDay(1))\n",
    "# Datum definieren\n",
    "my_date = datetime.date(2024, 1, 22)\n",
    "print(my_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_fdTbpD1LRi9"
   },
   "outputs": [],
   "source": [
    "# Datum aus String erzeugen, für DE und US\n",
    "# !pip install dateparser\n",
    "import dateparser\n",
    "date_string = \"Montag, 10. Juli 2023 08:42:25\"\n",
    "date_obj = dateparser.parse(date_string)\n",
    "# OR: date_obj = dateparser.parse(date_string, languages=['de'])\n",
    "\n",
    "# Datum / Uhrzeit als string\n",
    "from datetime import datetime\n",
    "current_date_time = datetime.now().strftime(\"%d.%m.%Y, %H:%M:%S\")\n",
    "# 15.08.2022, 17:49:46\n",
    "\n",
    "# Dauer messen\n",
    "import time\n",
    "start = time.time()\n",
    "print(\"hello\")\n",
    "time.sleep(1) # waits for 1 sec\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKh4M9VyL-i-"
   },
   "source": [
    "# Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_txXeYVKLRlY",
    "outputId": "862c19f5-5c45-4adb-8a79-9cf8b609fcf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "Find and replace me\n",
      "This is a demo text. <REPLACED!>. Lorem ipsum.\n",
      "\n",
      "This is a demo text. \n",
      "And a second line\n",
      "Lorem ipsum.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Finde Zahl in String, z. B. die \"123\" im folgenden String:\n",
    "#    project x/audio/chunks/segment_123.mp3\n",
    "# https://regex101.com/r/xwBbvc/1\n",
    "file_number_regex = re.findall(\"segment_(\\d+).mp3\", \"project x/audio/chunks/segment_123.mp3\")\n",
    "if file_number_regex:\n",
    "\tfile_number = file_number_regex[0] # 1st matching group\n",
    "\tprint(file_number)\n",
    "else:\n",
    "\tprint('Nothing found...')\n",
    "\n",
    "# Another demo\n",
    "# Find first occurance\n",
    "input_txt = \"This is a demo text. <Find and replace me>. Lorem ipsum.\"\n",
    "text_inside_tags = re.findall('(?sm)<(.+)>', input_txt)[0]\n",
    "print(text_inside_tags)\n",
    "# Replace using groups\n",
    "new_text = 'REPLACED!'\n",
    "replaced_txt = re.sub('(<)(.+)(>)', r'\\1' + new_text + r'\\3' , input_txt)\n",
    "print(replaced_txt)\n",
    "\n",
    "\n",
    "# Ersetze alle führenden SPACES zu Beginn von Zeilen:\n",
    "input_txt = \"\"\"\n",
    "  This is a demo text.\n",
    "\t   And a second line\n",
    "\t\t     Lorem ipsum.\n",
    "\"\"\"\n",
    "output_text = re.sub('\\n\\s{2,}', '\\n', input_txt)\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ig133Sw3MA5p"
   },
   "source": [
    "# Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBx2PHQiLRn-"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Array Funktionen\n",
    "``` Python\n",
    "# Loop through array with index (enumerate as \"helper\" for index)\n",
    "for index, part in enumerate(my_array):\n",
    "    print(index,part)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yoskB5hcMD4M"
   },
   "source": [
    "# Dictionary / Serializing (pickle)\n",
    "\n",
    "> Eingerückter Textblock\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AoITtxJuLRqN",
    "outputId": "a8f52c90-3753-41c5-e1ee-9898b41b39ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Dictionary: \n",
      "{'Name': 'Geeks', 1: [1, 2, 3, 4]}\n",
      "Accessing a element using key:\n",
      "Geeks\n",
      "Accessing a element using get:\n",
      "[1, 2, 3, 4]\n",
      "{1: 1, 2: 4, 3: 9, 4: 16, 5: 25}\n",
      "Access existing element \"Name\":\n",
      "Geeks\n",
      "Access non existing element, skipping error:\n"
     ]
    }
   ],
   "source": [
    "# Dictionary (ähnlich Javascript-Objekt/JSON)\n",
    "\n",
    "# Creating a Dictionary\n",
    "Dict = {'Name': 'Geeks', 1: [1, 2, 3, 4]}\n",
    "print(\"Creating Dictionary: \")\n",
    "print(Dict)\n",
    "\n",
    "# accessing a element using key - without or without default/fallback\n",
    "print(\"Accessing a element using key:\")\n",
    "print(Dict['Name'])\n",
    "print(Dict['Phone'], 'Sorry, no Phone in Dictionary')\n",
    "\n",
    "# accessing a element using get()\n",
    "# method\n",
    "print(\"Accessing a element using get:\")\n",
    "print(Dict.get(1))\n",
    "\n",
    "# creation using Dictionary comprehension\n",
    "myDict = {x: x**2 for x in [1,2,3,4,5]}\n",
    "print(myDict)\n",
    "\n",
    "# access dict and\n",
    "from contextlib import suppress\n",
    "print('Access existing element \"Name\":')\n",
    "with suppress(KeyError): print(Dict['Name'])\n",
    "print('Access non existing element, skipping error:')\n",
    "with suppress(KeyError): print(Dict['yoyo'])\n",
    "\n",
    "# convert/print Dictionary to JSON in formatted way\n",
    "print(json.dumps(myDict, sort_keys=True, indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0yB3Uz8RE6YY"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "data = {\n",
    "    \"1\": {\n",
    "        \"input\": \"chat input\",\n",
    "        \"output\": \"chat output\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# append data\n",
    "data.update({\"2\": {\n",
    "        \"input\": \"chat input2\",\n",
    "        \"output\": \"chat output2\"\n",
    "    }})\n",
    "\n",
    "# save data to file\n",
    "file = open('chat.dump', 'wb')\n",
    "pickle.dump(data, file)\n",
    "file.close()\n",
    "\n",
    "# read data from file\n",
    "file = open('chat.dump', 'rb')\n",
    "data2 = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "print(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVz6TyR8MIcq"
   },
   "source": [
    "# Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v38o9ZnlLRsq"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# File schreiben, lesen, kopieren, Pfade/Filenamen bearbeiten\n",
    "``` Python\n",
    "import io\n",
    "import os\n",
    "\n",
    "# Read full file\n",
    "f= open('testfile.txt','r')\n",
    "if f.mode == 'r':\n",
    "      contents =f.read()\n",
    "      print(contents)\n",
    "f.close()\n",
    "\n",
    "# Read file line-by-line\n",
    "f= open('testfile.txt','r')\n",
    "if f.mode == 'r':\n",
    "      f1 = f.readlines()\n",
    "      for x in f1:\n",
    "          print(x)\n",
    "f.close()\n",
    "\n",
    "# Write new file\n",
    "f= open('testfile.txt','w+')\n",
    "for i in range(10):\n",
    "     f.write('This is line %d\\r\\n' % (i+1))\n",
    "f.close()\n",
    "\n",
    "# Append to file\n",
    "f= open('testfile.txt','a+')\n",
    "f.write('this is appended text 1\\r\\n')\n",
    "f.write('this is appended text 2\\r\\n')\n",
    "f.close()\n",
    "\n",
    "# Delete file\n",
    "os.remove('testfile.txt')\n",
    "\n",
    "# Copy file\n",
    "import shutil\n",
    "shutil.copyfile(original_with_path, target_with_path)\n",
    "\n",
    "# Get filename from full path\n",
    "just_filename = os.path.basename(full_path_incl_filenmae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AIb4gq03heyv",
    "outputId": "e9ec682d-0ed7-447c-dba6-c3b262a54258"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./.config/.last_opt_in_prompt.yaml\n",
      "./.config/.last_update_check.json\n",
      "./.config/config_sentinel\n",
      "./.config/gce\n",
      "./.config/active_config\n",
      "./.config/.last_survey_prompt.yaml\n",
      "./.config/default_configs.db\n",
      "./.config/logs/2024.01.25/14.23.07.113867.log\n",
      "./.config/logs/2024.01.25/14.22.36.490195.log\n",
      "./.config/logs/2024.01.25/14.22.55.873995.log\n",
      "./.config/logs/2024.01.25/14.23.08.041697.log\n",
      "./.config/logs/2024.01.25/14.22.46.785493.log\n",
      "./.config/logs/2024.01.25/14.22.10.002730.log\n",
      "./.config/configurations/config_default\n",
      "./sample_data/README.md\n",
      "./sample_data/anscombe.json\n",
      "./sample_data/california_housing_train.csv\n",
      "./sample_data/mnist_test.csv\n",
      "./sample_data/mnist_train_small.csv\n",
      "./sample_data/california_housing_test.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create directory (if it not exists already)\n",
    "directory_name = 'test-folder'\n",
    "if not os.path.exists(directory_name):\n",
    "\tos.mkdir(directory_name)\n",
    "\n",
    "\n",
    "# Delete directory recursively (if it exists already)\n",
    "directory_name = 'test-folder'\n",
    "if os.path.exists(directory_name):\n",
    "    shutil.rmtree(directory_name)\n",
    "\n",
    "\n",
    "# Walk through directory, recursively incl. subdirectories\n",
    "directory_path = \".\"\n",
    "for root, _, files in os.walk(directory_path):\n",
    "    for file_name in files:\n",
    "        file_name_with_path = os.path.join(root, file_name)\n",
    "        print(file_name_with_path)\n",
    "        if file_name.endswith('.xlsx'):\n",
    "            print('Excel found: ', file_name_with_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ax1rrIMyMKg0"
   },
   "source": [
    "# ZIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kzPXVs5XLjXC"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ZIP (Archive, zip, unzip)\n",
    "``` Python\n",
    "# 1.\n",
    "# with shutil\n",
    "import shutil\n",
    "# zip\n",
    "shutil.make_archive('archive-name-without-.zip', 'zip', 'folder-which-should-be-zipped')\n",
    "# unzip\n",
    "shutil.unpack_archive('archive.zip', 'destination-folder')\n",
    "\n",
    "# 2.\n",
    "# with zipfile - example: Exclude folder for zip\n",
    "# https://datagy.io/python-zip-unzip-files/\n",
    "import os\n",
    "import zipfile\n",
    "# zip\n",
    "directory = \"folder-which-should-be-zipped\"\n",
    "with zipfile.ZipFile(\"zipfile.zip\", \"w\") as zip:\n",
    "    for subdir, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            srcpath = os.path.join(subdir, file)\n",
    "            if (subdir != 'exclude-folder1') and (subdir.startswith('exclude-folder2') == False):\n",
    "              #print(subdir)\n",
    "              dstpath_in_zip = os.path.relpath(srcpath, start=directory)\n",
    "              with open(srcpath, 'rb') as infile:\n",
    "                  print(srcpath)\n",
    "                  zip.writestr(dstpath_in_zip, infile.read())\n",
    "# unzip\n",
    "with zipfile.ZipFile('zipfile.zip', 'r') as zip:\n",
    "    zip.printdir()\n",
    "    zip.extractall('./')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0xPR5phy3Ib"
   },
   "source": [
    "# Webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J4byauGAy49D",
    "outputId": "491e10cc-1fac-4ecb-debf-5e195f38d21b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deutschland\n",
      "Hochwasserlage bleibt angespannt\n",
      "Stand: 30.12.2023 18:22 Uhr\n",
      "In einigen Landkreisen in Niedersachsen hat sich die Hochwasserlage etwas beruhigt, in anderen steigen die Pegelstände noch. Der Deutsche Wetterdienst rechnet zumindest für heute nicht mit neuem Regen in den betroffenen Gebieten.\n",
      "Die Hochwasserlage bleibt vor allem in Niedersachsen weiter kritisch. An einigen Pegeln der Weser befinden sich die Wasserstände noch über der höchsten Meldestufe, wie aus einem Lagebericht des\n",
      "Landesbetriebs für Wasserwirtschaft, Küsten- und Naturschutz (NLWKN)\n",
      "hervorgeht. Für die Leine, die Aller sowie die Ober- und Mittelweser gebe es eine Warnung vor großem Hochwasser. In\n",
      "Schladen\n",
      "im Landkreis Wolfenbüttel stieg der Pegelstand der Oker demnach um mehrere Zentimeter.\n",
      "An der Weser bei\n",
      "Drakenburg\n",
      "überschritt der Wasserstand mit 835 Zentimetern den bisherigen Höchstwert aus 1981 um einen Zentimeter, wie der Überregionale Hochwasserdienst mitteilte. \"Der Scheitel ist aber bereits erreicht worden und die Wasserstände am Pegel sinken leicht.\"\n",
      "hintergrund\n",
      "29.12.2023\n",
      "Hochwasser in Deutschland\n",
      "Warum es zurzeit viel regnet\n",
      "Der viele Regen hat dabei vor allem mit dem derzeit stark ausgeprägten Jetstream zu tun.\n",
      "mehr\n",
      "Oldenburg bereitet sich auf mögliche Evakuierung vor\n",
      "Daniel Jungnick, Leiter des Koordinierungsstabes des Technischen Hilfswerks (THW) in Bremen/Niedersachsen, sagte dem\n",
      "NDR\n",
      ", die Hochwasserlage verschiebe sich flussabwärts und verschärfe sich in den\n",
      "Landkreisen Celle, Heidekreis, Verden, Emsland\n",
      "und dem Gebiet um\n",
      "Oldenburg\n",
      ".\n",
      "In der Stadt\n",
      "Oldenburg\n",
      "wird eine mögliche Evakuierung vorbereitet. Die Deiche seien unverändert einem hohen Druck ausgesetzt, teilte die Stadt mit. Besonders betroffen ist den Angaben nach der Bereich Achterdiek, wo der Küstenkanal in die Hunte mündet. \"Es handelt sich hierbei um eine Vorsichtsmaßnahme - eine konkrete Evakuierung ist derzeit nicht vorgesehen\", hieß es in einer Mitteilung. Eine Notunterkunft stünde betroffenen Bürger zu Verfügung, hieß es.\n",
      "In\n",
      "Celle\n",
      "hat sich die Lage inzwischen aber etwas entspannt. In allen Bereichen sinken die Pegelstände leicht, wie die Verwaltung mitteilte. Es sei aber weiter die höchste Meldestufe an den Pegeln überschritten, so dass unverändert größere Überschwemmungen drohen. Man müsse \"weiter wachsam bleiben und die Lage genau verfolgen\", sagte Landrat Axel Flader.\n",
      "\"Das ist hier noch dramatisch\", Birgit Stamerjohanns, NDR, zur Hochwasserlage in Sandkrug/Landkreis Oldenburg\n",
      "tagesschau24, 30.12.2023 13:00 Uhr\n",
      "Leichte Entspannung im Serengeti-Park\n",
      "Auch im\n",
      "Serengeti-Park Hodenhagen\n",
      "beruhigte sich die Lage etwas. Pumpen hätten es geschafft, große Wassermengen hinter den Deich Richtung Meiße zu drücken, sagte eine Sprecherin des Parks. Auch im Tierhaus der Antilopen und Giraffen sei das Wasser merklich gesunken und aus dem Gebäude hinausgeflossen. Weite Teile des Geländes seien aber weiterhin überflutet und teilweise nicht zu erreichen.\n",
      "Laut NLKWN gibt es an vielen Orten eine gleichbleibende Tendenz bei den Pegeln. Für Hunderte Menschen in\n",
      "Lilienthal\n",
      "bei Bremen bedeutet das, dass sie weiter nicht in ihre Häuser zurückkönnen. Die Evakuierungen dauerten an, sagte eine Gemeindesprecherin. Die Lage bleibe angespannt.\n",
      "Hochwasserlage in weiten Teilen Nord- und Ostdeutschlands weiterhin angespannt\n",
      "Annette Deutskens, NDR, tagesschau, 30.12.2023 20:00 Uhr\n",
      "Viele Deiche massiv aufgeweicht\n",
      "Die Lagezentren der Polizei in Niedersachsen meldeten eine relativ ruhige Nacht. Einige Sprecher, wie vom Lagezentrum Göttingen, berichteten sogar von einer sich entspannenden Situation. Es sei trocken geblieben, daher gehe man davon aus, dass sich die Lage beruhige und Pegelstände sinken.\n",
      "In\n",
      "Meppen\n",
      "ist das bereits geschehen, wenn auch nur minimal. Es sei allerdings weiter Vorsicht geboten, teilte die Stadt mit. Die konkrete Entwicklung der zu erwartenden Pegelstände sei weiter ungewiss.\n",
      "In den\n",
      "Landkreisen Lüchow-Dannenberg und Verden\n",
      "erwarten die Verantwortlichen erst in der Neujahrswoche sinkende Pegelstände. Das THW stellt sich auf einen Einsatz in den Hochwassergebieten bis in die erste Januarwoche hinein ein. \"Es ist ganz klar, dass das über den Jahreswechsel andauern wird\", sagte THW-Präsidentin Sabine Lackner. \"Was uns hoch besorgt, ist der Zustand der Deiche.\" Sie seien massiv aufgeweicht. Täglich seien etwa 1.000 Einsatzkräfte in den betroffenen Gebieten unterwegs.\n",
      "Joop Wösten, NDR, über die aktuelle Lage und Auswirkungen eines Deich-Bruches im Emsland\n",
      "tagesschau24, 30.12.2023 19:00 Uhr\n",
      "Land sieht sich gut aufgestellt\n",
      "Niedersachsen sieht sich insgesamt gut aufgestellt mit Rettungskräften. Man gehe davon aus, dass man die Lage auch über Silvester mit eigenen Kräften bewältigen könne, sagte ein Sprecher des Innenministeriums der Nachrichtenagentur dpa. Das Land habe auch um Hilfe bei der Bundeswehr gebeten - dabei ging es bislang um sogenannte taktische Verlegungen, womit etwa Hubschrauber im Bedarfsfall schneller vor Ort sein können.\n",
      "04.01.2024\n",
      "Niedersachsen\n",
      "Liveticker zum Hochwasser in Niedersachsen: Auch Grundwasser steigt\n",
      "Experten warnen vor Überflutungen abseits der betroffenen Gewässer.\n",
      "mehr\n",
      "Weiter Störungen im Bahnverkehr\n",
      "Der Deutsche Wetterdienst (DWD) erwartet für die niedersächsischen Hochwassergebiete am Samstag keinen neuen Regen. Abseits der Küste soll es nur vereinzelt und in geringen Mengen Schauer geben. Zwischen Sonntag und Montag kommt es im Land aber voraussichtlich verbreitet wieder zu Niederschlägen, meist zwischen einem und fünf Litern Regen pro Quadratmeter. Diese Menge wird laut DWD aber nicht zu einem Anstieg der Pegel führen. Erst ab Dienstag würden wieder größere Niederschlagsmengen erwartet.\n",
      "Aufgrund der Witterung und des Hochwassers müssen sich Bahnreisende länger als geplant auf Verspätungen und Streckensperrungen einstellen. Die\n",
      "Verbindung zwischen Oldenburg und Osnabrück\n",
      "sei wegen des Hochwassers nach wie vor eingeschränkt, sagte eine Sprecherin der Nordwestbahn.\n",
      "Wasser in Sachsen-Anhalt steigt wieder\n",
      "Auch\n",
      "Sachsen-Anhalt\n",
      "ist weiterhin von Hochwasser betroffen. Regen führte in der vergangenen Nacht dazu, dass einige Flüsse wieder anstiegen. Die Niederschläge seien stärker ausgefallen als zunächst prognostiziert, teilte der Landesbetrieb für Hochwasserschutz (LHW) mit. Weil die Böden bereits gesättigt seien, habe dies in einigen Bereichen zu ansteigenden Wasserständen geführt. So sei an der Dumme in der Altmark der Richtwert der Alarmstufe 2 überschritten worden, nachdem sich die Lage dort in den vergangenen Tagen zunächst entspannt hatte.\n",
      "Der Wasserstand im Fluss Helme stieg ebenfalls an. Der Pegelstand lag in Bennungen knapp einen halben Meter über dem Richtwert von zwei Metern für die höchste Alarmstufe, nachdem der Abfluss aus der\n",
      "Talsperre Kelbra\n",
      "erhöht wurde. Die Lage dort werde immer kritischer, sagte der Bürgermeister der Gemeinde Südharz, Peter Koh. Auch an Jeetze und Aland gebe es eine leicht steigende Tendenz der Pegelstände.\n",
      "Hochwasser in Sachsen geht weiter zurück\n",
      "An den\n",
      "Talsperren im Harz\n",
      "sinken die Füllstände dagegen weiter. Derzeit wird dort nicht mehr Wasser über den Notüberlauf abgegeben, wie ein Sprecher der Harzwasserwerke sagte. Die Lage sei allerdings weiter angespannt, da noch immer zu viel Wasser in den Reservoirs sei. Die Harzwasserwerke hoffen auf trockenes Wetter, um die Talsperren weiter ablassen und dadurch den Hochwasserschutz gewährleisten zu können.\n",
      "Auch aus\n",
      "Sachsen\n",
      "gibt es Signale der Entspannung: Das Hochwasser der Elbe geht weiter zurück. Am Pegel\n",
      "Dresden\n",
      "wurde am Samstagmorgen ein Wasserstand von 5,30 Meter gemessen, wie aus einer Übersicht des Landeshochwasserzentrums hervorging. Einen Tag zuvor waren es noch 5,92 Meter gewesen. Normal sind rund zwei Meter. In der Landeshauptstadt galt ebenso wie in\n",
      "Schöna\n",
      "an der tschechischen Grenze sowie flussabwärts in\n",
      "Riesa\n",
      "noch die Alarmstufe 2. Die Hydrologen rechnen mit weiter sinkenden Wasserständen. Für die übrigen Flussgebiete in Sachsen gab es keine Hochwasserwarnungen mehr.\n",
      "04.01.2024\n",
      "Sachsen-Anhalt\n",
      "Scholz lobt Solidarität in Mansfeld-Südharz\n",
      "Bundeskanzler Scholz hat am Mittag die vom Hochwasser betroffenen Gebiete im Südharz besucht.\n",
      "mehr\n",
      "Katastrophentouristen\n",
      "bereiten Probleme\n",
      "In Niedersachsen bereitet den Behörden unterdessen \"wachsender Hochwassertourismus\" Probleme. Die Stadt\n",
      "Celle\n",
      "appellierte an Menschen, Sperrungen ernst zu nehmen und nur in die Stadt zu reisen, wenn es unbedingt notwendig sei. Rettungskräfte würden vielerorts am Durchkommen gehindert. Auch die Feuerwehr Verden berichtete von störenden Katastrophentouristen.\n",
      "Der\n",
      "Landkreis Osterholz\n",
      "befürchtet darüber hinaus, dass zu Silvester viele Schaulustige im Hochwassergebiet unterwegs sein werden. Zahlreiche Landkreise appellierten erneut, Deiche nicht zu betreten, da diese aufgeweicht seien und beschädigt werden könnten. In der Stadt\n",
      "Oldenburg\n",
      "gilt ein Betretungsverbot für Deiche, das mit bis zu 5.000 Euro geahndet wird.\n",
      "Hochwasser\n",
      "Deutschland\n",
      "Dieses Thema im Programm:\n",
      "Über dieses Thema berichtete tagesschau24 am 30. Dezember 2023 um 09:00 Uhr.\n",
      "Facebook\n",
      "Facebook\n",
      "X\n",
      "X\n",
      "Whatsapp\n",
      "WhatsApp\n",
      "Mail\n",
      "Mail\n",
      "Drucken\n",
      "Drucken\n"
     ]
    }
   ],
   "source": [
    "#!pip install bs4\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "response = requests.get(\"https://www.tagesschau.de/inland/hochwasser-deutschland-126.html\")\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "all_text = soup.get_text(separator='\\n', strip=True)\n",
    "div_text=soup.find(\"div\",{\"class\":\"layout-container\"}).get_text()\n",
    "article_text=soup.find(\"article\").get_text(separator='\\n', strip=True)\n",
    "print(article_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HxzZqlVML1j"
   },
   "source": [
    "# Office - Powerpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_qs9GalLLjZL"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Powerpoint Editieren\n",
    "``` Python\n",
    "# see my Notebook: https://github.com/aknip/Streamlit-Gradio/blob/main/Powerpoint%20and%20PDF%20Generation.ipynb\n",
    "\n",
    "# Source: \"Using Python to Update PowerPoint. Step by step tutorial to edit PowerPoint slides using Python\": https://towardsdatascience.com/use-python-to-automate-the-powerpoint-update-4a385acf1243\n",
    "\n",
    "# Setup:\n",
    "# pip install python-pptx\n",
    "\n",
    "# Fonts:\n",
    "# sudo apt install fonts-open-sans\n",
    "# sudo apt install fonts-roboto\n",
    "\n",
    "from pptx import Presentation\n",
    "my_ppt = Presentation('ppt-input/presentation.pptx')\n",
    "# do something with python-pptx\n",
    "my_ppt.save('ppt-output/presentation2.pptx')\n",
    "```\n",
    "\n",
    "\n",
    "# Powerpoint (Office) zu PDF Konvertieren\n",
    "``` Python\n",
    "# see my Notebook: https://github.com/aknip/Streamlit-Gradio/blob/main/Powerpoint%20and%20PDF%20Generation.ipynb\n",
    "\n",
    "# Source: \"Stackoverflow: Use unoconv on Ubuntu to convert PPTX to PDF\": https://stackoverflow.com/a/63664087\n",
    "\n",
    "# Setup:\n",
    "# sudo apt install unoconv\n",
    "# pip install tqdm\n",
    "\t# pip install glob\n",
    "\n",
    "# Fonts:\n",
    "# sudo apt install fonts-open-sans\n",
    "# sudo apt install fonts-roboto\n",
    "\n",
    "command = 'unoconv -f pdf -o \"pdf-output/presentation.pdf\" \"ppt-input/presentation.pptx\" '\n",
    "os.system(command)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Office - Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install python-docx==1.1.2\n",
    "\n",
    "import docx\n",
    "\n",
    "document = docx.Document()\n",
    "\n",
    "document.add_paragraph(\"Table 1\")\n",
    "\n",
    "table = document.add_table(rows=1, cols=2)\n",
    "table.style = 'Table Grid'\n",
    "last_row = table.rows[-1]\n",
    "for cell in last_row.cells: \n",
    "    cell.text = 'Cell Header'\n",
    "\n",
    "table.add_row()\n",
    "last_row = table.rows[-1]\n",
    "for cell in last_row.cells: \n",
    "    cell.text = 'Cell Data'\n",
    "\n",
    "table.add_row()\n",
    "last_row = table.rows[-1]\n",
    "for cell in last_row.cells: \n",
    "    cell.text = 'Cell Data'\n",
    "\n",
    "document.add_paragraph(\"Table 2\")\n",
    "table2 = document.add_table(rows=3, cols=3)\n",
    "table2.style = 'Table Grid'\n",
    "for row in table2.rows: \n",
    "    for cell in row.cells: \n",
    "        cell.text = 'Cell Data'\n",
    "        \n",
    "document.save('test.docx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FKeeFglMNz7"
   },
   "source": [
    "# Office - Word - Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cnp5buvILM1q"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Markdown / Word konvertieren, beide Richtungen (Pandoc / pypandoc)\n",
    "``` Python\n",
    "#pip install pypandoc\n",
    "import pypandoc\n",
    "# md to docx\n",
    "docx_file = pypandoc.convert_file('input_file.md', 'docx', outputfile='output_file.docx')\n",
    "# docx to md\n",
    "md_file = pypandoc.convert_file('input_file.docx', 'md', outputfile='output_file.md')\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "# Word zu Markdown konvertieren\n",
    "- Ideen/Quellen:\n",
    "\t- https://medium.com/geekculture/how-to-easily-convert-word-to-markdown-with-pandoc-4d60878ccc64\n",
    "\t- https://towardsdatascience.com/word-document-to-html-or-markdown-with-python-37db7150258c\n",
    "\t-\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hEzIITILV_fm"
   },
   "source": [
    "# Email Conversion .msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "koG0NZlYLjbm"
   },
   "outputs": [],
   "source": [
    "!pip install extract-msg\n",
    "import extract_msg\n",
    "msg = extract_msg.openMsg(\"./msg/test-email.msg\")\n",
    "# or (better:)\n",
    "!python3 -m extract_msg \"msg/test-email.msg\" --out msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUKjXKfX2CLD"
   },
   "source": [
    "# Convert CSV to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E6xFn3D519yP",
    "outputId": "82c563ac-7345-42a0-a0d8-c416b0534106"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m{\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"destination\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"Paris\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"country\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"France\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"attractions\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"Eiffel Tower, Louvre Museum\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m},\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m{\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"destination\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"New York City\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"country\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"United States\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"attractions\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"Times Square, Statue of Liberty, Broadway shows\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m},\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m{\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"destination\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"Kyoto\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"country\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"Japan\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"attractions\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"ancient temples, tranquil gardens, traditional geisha culture\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m},\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m{\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"destination\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"Great Wall of China\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"country\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"China\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"attractions\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34mnull\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m},\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m{\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"destination\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"Rio de Janeiro\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"country\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"Brazil\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"attractions\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"Carnival celebrations, Copacabana Beach, Christ the Redeemer statue\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m},\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m{\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"destination\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"Great Barrier Reef\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"country\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"Australia\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"attractions\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"stunning coral reefs, diverse marine life\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m},\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m{\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"destination\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"Sydney Opera House\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"country\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"Australia\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"attractions\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"architectural brilliance\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m}\u001b[37m\u001b[39;49;00m\n",
      "]\u001b[37m\u001b[39;49;00m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from io import StringIO\n",
    "from pygments import highlight, lexers, formatters\n",
    "\n",
    "CSV_string = \"\"\"destination|country|attractions\n",
    "Paris|France|Eiffel Tower, Louvre Museum\n",
    "New York City|United States|Times Square, Statue of Liberty, Broadway shows\n",
    "Kyoto|Japan|ancient temples, tranquil gardens, traditional geisha culture\n",
    "Great Wall of China|China|\n",
    "Rio de Janeiro|Brazil|Carnival celebrations, Copacabana Beach, Christ the Redeemer statue\n",
    "Great Barrier Reef|Australia|stunning coral reefs, diverse marine life\n",
    "Sydney Opera House|Australia|architectural brilliance\n",
    "\"\"\"\n",
    "\n",
    "CSV_string = CSV_string.replace(\"'\", '\"')\n",
    "response_IO = StringIO(CSV_string) # [4:] strip away first 4 chars \"AI: \"\n",
    "response_df = pd.read_csv(response_IO, sep=\"|\")\n",
    "response_JSON = json.loads(response_df.to_json(orient='table',index=False))['data']\n",
    "\n",
    "formatted_json = json.dumps(response_JSON, indent=4)\n",
    "colorful_json = highlight(formatted_json,\n",
    "                          lexers.JsonLexer(),\n",
    "                          formatters.TerminalFormatter())\n",
    "print(colorful_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM85My7R3UEADSB86sCieeK",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
